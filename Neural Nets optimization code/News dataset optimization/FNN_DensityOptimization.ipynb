{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'.\\News_Dataset_Splits\\X_train.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "with open(r'.\\News_Dataset_Splits\\X_val.pkl', 'rb') as f:\n",
    "    X_val = pickle.load(f)\n",
    "\n",
    "with open(r'.\\News_Dataset_Splits\\X_test.pkl', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "\n",
    "with open(r'.\\News_Dataset_Splits\\y_train.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "with open(r'.\\News_Dataset_Splits\\y_val.pkl', 'rb') as f:\n",
    "    y_val = pickle.load(f)\n",
    "\n",
    "with open(r'.\\News_Dataset_Splits\\y_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(np.unique(y_train))\n",
    "train_labels = encoder.transform(y_train)\n",
    "val_labels = encoder.transform(y_val)\n",
    "test_labels = encoder.transform(y_test)\n",
    "num_classes = len(encoder.classes_)\n",
    "train_one_hot = keras.utils.to_categorical(train_labels, num_classes=num_classes)\n",
    "val_one_hot = keras.utils.to_categorical(val_labels, num_classes=num_classes)\n",
    "test_one_hot = keras.utils.to_categorical(test_labels, num_classes=num_classes)\n",
    "\n",
    "with open(r'.\\embeddingMatrix_News.pkl', 'rb') as f:\n",
    "    embedding_matrix = pickle.load(f)\n",
    "\n",
    "num_tokens = len(embedding_matrix) # total vocabulary +1 or length of embedding matrix\n",
    "embedding_dim = 300 # dimension of the vector of a single word\n",
    "MAX_NEWS_LEN = 500 # maximum words in a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp, max_layers, test_optimizers, test_initializers, test_regularizer, test_activations, use_BatchNormalization, \n",
    "                use_Dropout, units_min_value, units_max_value, units_step, same_units=False):\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        input_length=MAX_NEWS_LEN,\n",
    "        trainable=True)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(keras.layers.GlobalAveragePooling1D())\n",
    "\n",
    "    num_layers = hp.Int('num_layers', 1, max_layers)\n",
    "\n",
    "    # Determine the number of units for each layer\n",
    "    if same_units:\n",
    "        units = hp.Int('units', min_value=units_min_value, max_value=units_max_value, step=units_step)\n",
    "\n",
    "    # Hyperparameters for the number of layers\n",
    "    for i in range(num_layers):\n",
    "        if same_units:\n",
    "            current_units = units\n",
    "        else:\n",
    "            current_units = hp.Int(f'units_{i}', min_value=units_min_value, max_value=units_max_value, step=units_step)\n",
    "\n",
    "        if test_activations:\n",
    "            activation = hp.Choice(f'activation_{i}', ['softmax', 'elu', 'selu', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'exponential', 'linear'])\n",
    "        else:\n",
    "            activation = 'relu'  # Default activation\n",
    "\n",
    "        if test_initializers:\n",
    "            kernel_initializer = hp.Choice(f'kernel_initializer_{i}', ['glorot_uniform', 'he_uniform', 'lecun_uniform'])\n",
    "        else:\n",
    "            kernel_initializer = 'glorot_uniform'\n",
    "        \n",
    "        if test_regularizer:\n",
    "            kernel_regularizer = hp.Choice(f'kernel_regularizer_{i}', [None, tf.keras.regularizers.l1(), tf.keras.regularizers.l2(), tf.keras.regularizers.l1_l2()])\n",
    "        else:\n",
    "            kernel_regularizer = None\n",
    "\n",
    "        model.add(layers.Dense(\n",
    "            units=current_units,\n",
    "            activation=activation,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            kernel_regularizer=kernel_regularizer\n",
    "        ))\n",
    "\n",
    "        if use_BatchNormalization and hp.Boolean(f'batch_norm_{i}'):\n",
    "            model.add(layers.BatchNormalization())\n",
    "\n",
    "        if use_Dropout and hp.Boolean(f'dropout_{i}'):\n",
    "            model.add(layers.Dropout(rate=hp.Float(f'dropout_rate_{i}', 0.1, 0.5, step=0.1)))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Optimizer\n",
    "    if test_optimizers:\n",
    "        optimizer = hp.Choice('optimizer', ['SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam'])\n",
    "    else:\n",
    "        optimizer = 'adam'\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_s(hp, test_optimizers):\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        input_length=MAX_NEWS_LEN,\n",
    "        trainable=True)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(keras.layers.GlobalAveragePooling1D())\n",
    "\n",
    "    model.add(layers.Dense(units=64, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(layers.Dense(units=64, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(layers.Dense(units=96, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Optimizer\n",
    "    if test_optimizers:\n",
    "        optimizer = hp.Choice('optimizer', ['SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam'])\n",
    "    else:\n",
    "        optimizer = 'adam'\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 15\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "directory = f'KerasTuner_Logs/FNN_V2_3Layer_Optimizer_{now}'\n",
    "\n",
    "# Callbacks\n",
    "tensorboard = TensorBoard(log_dir=f'TensorBoard_Logs/FNN_V2_3Layer_Optimizer_{now}')\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tuner = kt.GridSearch(\n",
    "    lambda hp: build_model_s(hp, test_optimizers=True),\n",
    "    objective=kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "    max_trials=None,\n",
    "    executions_per_trial=1,\n",
    "    directory=directory,\n",
    "    project_name='Reviews_Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 15\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "directory = f'KerasTuner_Logs/FNN_V2_1Layer_Optimizer_{now}'\n",
    "\n",
    "# Callbacks\n",
    "tensorboard = TensorBoard(log_dir=f'TensorBoard_Logs/FNN_V2_1Layer_Optimizer_{now}')\n",
    "'''model_checkpoint = ModelCheckpoint(\n",
    "    filepath=f\"SavedModels/{directory}/best_model.h5\",\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")'''\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tuner = kt.GridSearch(\n",
    "    lambda hp: build_model(hp, max_layers=1, test_optimizers=True, test_initializers=False, test_regularizer=False, test_activations=False,\n",
    "                           use_BatchNormalization=False, use_Dropout=False, units_min_value=128, units_max_value=128, units_step=128, same_units=True),\n",
    "    objective=kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "    max_trials=None,\n",
    "    executions_per_trial=1,\n",
    "    directory=directory,\n",
    "    project_name='Reviews_Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 Complete [00h 03m 00s]\n",
      "val_loss: 0.5447064638137817\n",
      "\n",
      "Best val_loss So Far: 0.5410788059234619\n",
      "Total elapsed time: 00h 22m 17s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x=X_train,\n",
    "             y=train_one_hot,\n",
    "             verbose=1,\n",
    "             epochs=NUM_EPOCHS,\n",
    "             batch_size=BATCH_SIZE,\n",
    "             callbacks=[tensorboard, early_stopping],\n",
    "             validation_data=(X_val, val_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer': 'RMSprop'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 300)          145070400 \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 300)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                19264     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 96)                6240      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                970       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 145,101,034\n",
      "Trainable params: 145,101,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.momentum\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.rho\n"
     ]
    }
   ],
   "source": [
    "tuner.get_best_models()[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in dir_2024-07-05_11-38-27\\Reviews_Classification\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 0001 summary\n",
      "Hyperparameters:\n",
      "units: 85\n",
      "num_layers: 1\n",
      "optimizer: Adam\n",
      "Score: 1.103798508644104\n",
      "\n",
      "Trial 0003 summary\n",
      "Hyperparameters:\n",
      "units: 85\n",
      "num_layers: 2\n",
      "optimizer: Adam\n",
      "Score: 1.1182894706726074\n",
      "\n",
      "Trial 0000 summary\n",
      "Hyperparameters:\n",
      "units: 85\n",
      "num_layers: 1\n",
      "optimizer: RMSprop\n",
      "Score: 1.1338261365890503\n",
      "\n",
      "Trial 0002 summary\n",
      "Hyperparameters:\n",
      "units: 85\n",
      "num_layers: 2\n",
      "optimizer: RMSprop\n",
      "Score: 1.1509822607040405\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from keras.models import load_model\n",
    "\n",
    "# Specify the path to your .h5 model file\n",
    "model_path = 'SavedModels/dir_2024-07-04_16-19-08/best_model.h5'\n",
    "\n",
    "# Load the model\n",
    "s_model = load_model(model_path)\n",
    "s_model.optimizer.get_config()'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
