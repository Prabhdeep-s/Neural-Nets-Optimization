{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'.\\News_Dataset_Splits\\X_train.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "with open(r'.\\News_Dataset_Splits\\X_val.pkl', 'rb') as f:\n",
    "    X_val = pickle.load(f)\n",
    "\n",
    "with open(r'.\\News_Dataset_Splits\\X_test.pkl', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "\n",
    "with open(r'.\\News_Dataset_Splits\\y_train.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "with open(r'.\\News_Dataset_Splits\\y_val.pkl', 'rb') as f:\n",
    "    y_val = pickle.load(f)\n",
    "\n",
    "with open(r'.\\News_Dataset_Splits\\y_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(np.unique(y_train))\n",
    "train_labels = encoder.transform(y_train)\n",
    "val_labels = encoder.transform(y_val)\n",
    "test_labels = encoder.transform(y_test)\n",
    "num_classes = len(encoder.classes_)\n",
    "train_one_hot = keras.utils.to_categorical(train_labels, num_classes=num_classes)\n",
    "val_one_hot = keras.utils.to_categorical(val_labels, num_classes=num_classes)\n",
    "test_one_hot = keras.utils.to_categorical(test_labels, num_classes=num_classes)\n",
    "\n",
    "with open(r'.\\embeddingMatrix_News.pkl', 'rb') as f:\n",
    "    embedding_matrix = pickle.load(f)\n",
    "\n",
    "num_tokens = len(embedding_matrix) # total vocabulary +1 or length of embedding matrix\n",
    "embedding_dim = 300 # dimension of the vector of a single word\n",
    "MAX_NEWS_LEN = 500 # maximum words in a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp, min_layers, max_layers, test_optimizers, test_activations,\n",
    "                units_min_value, units_max_value, units_step):\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        input_length=MAX_NEWS_LEN,\n",
    "        trainable=True,\n",
    "        mask_zero=True)\n",
    "   \n",
    "    model = keras.Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    \n",
    "    num_layers = hp.Int('num_layers', min_layers, max_layers)\n",
    "\n",
    "    if test_activations:\n",
    "            activation = hp.Choice(f'activation', ['softplus', 'softsign', 'relu', 'tanh'])\n",
    "    else:\n",
    "        activation = 'tanh'  # Default activation\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f'units_{i}', min_value=units_min_value, max_value=units_max_value, step=units_step)\n",
    "\n",
    "        if i < num_layers - 1:  # For all layers except the last one\n",
    "            model.add(keras.layers.LSTM(units, activation=activation, return_sequences=True))\n",
    "        else:  # For the last LSTM layer\n",
    "            model.add(keras.layers.LSTM(units, activation=activation))\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Optimizer\n",
    "    if test_optimizers:\n",
    "        optimizer = hp.Choice('optimizer', ['SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam'])\n",
    "    else:\n",
    "        optimizer = 'adam'\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 15\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "directory = f'KerasTuner_Logs/RNN/RNN_V1_3Layer_Density_Optimization_{now}'\n",
    "\n",
    "# Callbacks\n",
    "tensorboard = TensorBoard(log_dir=f'TensorBoard_Logs/RNN/RNN_V1_3Layer_Density_Optimization_{now}')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "\n",
    "tuner = kt.GridSearch(\n",
    "    lambda hp: build_model(hp, min_layers=3, max_layers=3, test_optimizers=False, test_activations=False, \n",
    "                           units_min_value=64, units_max_value=128, units_step=32),\n",
    "    objective=kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "    max_trials=None,\n",
    "    executions_per_trial=1,\n",
    "    directory=directory,\n",
    "    project_name='Reviews_Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 27 Complete [00h 23m 41s]\n",
      "val_loss: 0.5366922616958618\n",
      "\n",
      "Best val_loss So Far: 0.5204403400421143\n",
      "Total elapsed time: 10h 41m 53s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x=X_train,\n",
    "             y=train_one_hot,\n",
    "             verbose=1,\n",
    "             epochs=NUM_EPOCHS,\n",
    "             batch_size=BATCH_SIZE,\n",
    "             callbacks=[tensorboard, early_stopping],\n",
    "             validation_data=(X_val, val_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_layers': 3, 'units_0': 96, 'units_1': 96, 'units_2': 128}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 300)          145070400 \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 500, 128)          219648    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 145,422,922\n",
      "Trainable params: 145,422,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    }
   ],
   "source": [
    "tuner.get_best_models()[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in KerasTuner_Logs/RNN/RNN_V1_2Layer_Density_Optimization_2024-07-16_15-45-03\\Reviews_Classification\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 0000 summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_{i}: 64\n",
      "Score: 1.0833885669708252\n",
      "\n",
      "Trial 0002 summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_{i}: 128\n",
      "Score: 1.0934594869613647\n",
      "\n",
      "Trial 0001 summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_{i}: 96\n",
      "Score: 1.0941708087921143\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
