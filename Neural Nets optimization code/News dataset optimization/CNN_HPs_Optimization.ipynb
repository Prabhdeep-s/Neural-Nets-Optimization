{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'.\\News_Dataset_Splits\\X_train.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "with open(r'.\\News_Dataset_Splits\\X_val.pkl', 'rb') as f:\n",
    "    X_val = pickle.load(f)\n",
    "\n",
    "with open(r'.\\News_Dataset_Splits\\X_test.pkl', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "\n",
    "with open(r'.\\News_Dataset_Splits\\y_train.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "with open(r'.\\News_Dataset_Splits\\y_val.pkl', 'rb') as f:\n",
    "    y_val = pickle.load(f)\n",
    "\n",
    "with open(r'.\\News_Dataset_Splits\\y_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(np.unique(y_train))\n",
    "train_labels = encoder.transform(y_train)\n",
    "val_labels = encoder.transform(y_val)\n",
    "test_labels = encoder.transform(y_test)\n",
    "num_classes = len(encoder.classes_)\n",
    "train_one_hot = keras.utils.to_categorical(train_labels, num_classes=num_classes)\n",
    "val_one_hot = keras.utils.to_categorical(val_labels, num_classes=num_classes)\n",
    "test_one_hot = keras.utils.to_categorical(test_labels, num_classes=num_classes)\n",
    "\n",
    "with open(r'.\\embeddingMatrix_News.pkl', 'rb') as f:\n",
    "    embedding_matrix = pickle.load(f)\n",
    "\n",
    "num_tokens = len(embedding_matrix) # total vocabulary +1 or length of embedding matrix\n",
    "embedding_dim = 300 # dimension of the vector of a single word\n",
    "MAX_NEWS_LEN = 500 # maximum words in a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp, min_layers, max_layers, test_optimizers, test_initializers, test_regularizer, regularizer_choice, test_learning_rate,\n",
    "                test_activations, use_BatchNormalization, use_Dropout, filters_min_value, filters_max_value, filters_step, same_filters):\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        input_length=MAX_NEWS_LEN,\n",
    "        trainable=True)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(embedding_layer)\n",
    "\n",
    "    num_layers = hp.Int('num_layers', min_layers, max_layers)\n",
    "\n",
    "    if test_activations:\n",
    "            activation = hp.Choice(f'activation', ['softplus', 'softsign', 'relu', 'tanh'])\n",
    "    else:\n",
    "        activation = 'relu'  # Default activation\n",
    "\n",
    "    if test_initializers:\n",
    "        kernel_initializer = hp.Choice(f'kernel_initializer', ['glorot_uniform', 'he_uniform', 'random_uniform'])\n",
    "    else:\n",
    "        kernel_initializer = 'he_uniform'\n",
    "\n",
    "    if test_regularizer:\n",
    "        if regularizer_choice == 'l1':\n",
    "            kernel_regularizer = keras.regularizers.L1(l1=hp.Choice('l1_factor', [1e-4, 1e-2]))\n",
    "        elif regularizer_choice == 'l2':\n",
    "            kernel_regularizer = keras.regularizers.L2(l2=hp.Choice('l2_factor', [1e-4, 1e-2]))\n",
    "        elif regularizer_choice == 'l1_l2':\n",
    "            kernel_regularizer = keras.regularizers.L1L2(l1=hp.Choice('l1_l2_l1_factor', [1e-4, 1e-2]),\n",
    "                                    l2=hp.Choice('l1_l2_l2_factor', [1e-4, 1e-2]))\n",
    "    else:\n",
    "        kernel_regularizer = None\n",
    "\n",
    "    # Hyperparameters for the number of layers\n",
    "    for i in range(num_layers):\n",
    "        if same_filters:\n",
    "            filters = filters_min_value\n",
    "        else:\n",
    "            filters = hp.Int(f'filters_{i}', min_value=filters_min_value, max_value=filters_max_value, step=filters_step)\n",
    "\n",
    "        model.add(layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=5,\n",
    "            activation=activation,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            padding='same'\n",
    "        ))\n",
    "\n",
    "        if use_BatchNormalization and hp.Boolean(f'batch_norm_{i}'):\n",
    "            model.add(layers.BatchNormalization())\n",
    "\n",
    "        model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "        if use_Dropout:\n",
    "            model.add(layers.Dropout(rate=hp.Choice(f'dropout_rate_{i}', [0.0, 0.2, 0.4])))\n",
    "\n",
    "        model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Optimizer\n",
    "    if test_optimizers:\n",
    "        optimizer = hp.Choice('optimizer', ['SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam'])\n",
    "    else:\n",
    "        if test_learning_rate:\n",
    "            learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4, 1e-5])\n",
    "        else:\n",
    "            learning_rate = 0.001\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 12\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "directory = f'KerasTuner_Logs/CNN/CNN_V2_3Layer_Optimizer_{now}'\n",
    "\n",
    "# Callbacks\n",
    "tensorboard = TensorBoard(log_dir=f'TensorBoard_Logs/CNN/CNN_V2_3Layer_Optimizer_{now}')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "\n",
    "tuner = kt.GridSearch(\n",
    "    lambda hp: build_model(hp, min_layers=3, max_layers=3, test_optimizers=True, test_initializers=False, test_regularizer=False, \n",
    "                           regularizer_choice='l1', test_learning_rate=False, test_activations=False, use_BatchNormalization=False, \n",
    "                           use_Dropout=False, filters_min_value=128, filters_max_value=128, filters_step=128, same_filters=True),\n",
    "    objective=kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "    max_trials=None,\n",
    "    executions_per_trial=1,\n",
    "    directory=directory,\n",
    "    project_name='Reviews_Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pd(hp, test_optimizers, test_initializers, test_regularizer, regularizer_choice, test_learning_rate,\n",
    "                test_activations, use_BatchNormalization, use_Dropout):\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        input_length=MAX_NEWS_LEN,\n",
    "        trainable=True)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(embedding_layer)\n",
    "\n",
    "    if test_activations:\n",
    "            activation = hp.Choice(f'activation', ['softplus', 'softsign', 'relu', 'tanh'])\n",
    "    else:\n",
    "        activation = 'softplus'  # Default activation\n",
    "\n",
    "    if test_initializers:\n",
    "        kernel_initializer = hp.Choice(f'kernel_initializer', ['glorot_uniform', 'he_uniform', 'random_uniform'])\n",
    "    else:\n",
    "        kernel_initializer = 'random_uniform'\n",
    "\n",
    "    if test_regularizer:\n",
    "        if regularizer_choice == 'l1':\n",
    "            kernel_regularizer = keras.regularizers.L1(l1=hp.Choice('l1_factor', [1e-4, 1e-2]))\n",
    "        elif regularizer_choice == 'l2':\n",
    "            kernel_regularizer = keras.regularizers.L2(l2=hp.Choice('l2_factor', [1e-4, 1e-2]))\n",
    "        elif regularizer_choice == 'l1_l2':\n",
    "            kernel_regularizer = keras.regularizers.L1L2(l1=hp.Choice('l1_l2_l1_factor', [1e-4, 1e-2]),\n",
    "                                    l2=hp.Choice('l1_l2_l2_factor', [1e-4, 1e-2]))\n",
    "    else:\n",
    "        kernel_regularizer = None\n",
    "\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=5, activation=activation,\n",
    "            kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer, padding='same'))\n",
    "    if use_BatchNormalization and hp.Boolean(f'batch_norm'):\n",
    "            model.add(layers.BatchNormalization())\n",
    "    if use_Dropout:\n",
    "            model.add(layers.Dropout(rate=hp.Choice(f'dropout_rate_', [0.0, 0.2, 0.4])))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Optimizer\n",
    "    if test_optimizers:\n",
    "        optimizer = hp.Choice('optimizer', ['SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam'])\n",
    "    else:\n",
    "        if test_learning_rate:\n",
    "            learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4, 1e-5])\n",
    "        else:\n",
    "            learning_rate = 0.001\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 12\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "directory = f'KerasTuner_Logs/CNN/CNN_V7_1Layer_Dropout_{now}'\n",
    "\n",
    "# Callbacks\n",
    "tensorboard = TensorBoard(log_dir=f'TensorBoard_Logs/CNN/CNN_V7_1Layer_Dropout_{now}')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "\n",
    "tuner = kt.GridSearch(\n",
    "    lambda hp: build_model_pd(hp, test_optimizers=False, test_initializers=False, test_regularizer=False, \n",
    "                           regularizer_choice='l1_l2', test_learning_rate=False, test_activations=False, use_BatchNormalization=False, use_Dropout=True),\n",
    "    objective=kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "    max_trials=None,\n",
    "    executions_per_trial=1,\n",
    "    directory=directory,\n",
    "    project_name='Reviews_Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 04m 51s]\n",
      "val_loss: 0.6041333079338074\n",
      "\n",
      "Best val_loss So Far: 0.5988964438438416\n",
      "Total elapsed time: 00h 14m 29s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x=X_train,\n",
    "             y=train_one_hot,\n",
    "             verbose=1,\n",
    "             epochs=NUM_EPOCHS,\n",
    "             batch_size=BATCH_SIZE,\n",
    "             callbacks=[tensorboard, early_stopping],\n",
    "             validation_data=(X_val, val_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout_rate_': 0.2}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 250, 300)          16058400  \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 300)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               38528     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,097,573\n",
      "Trainable params: 16,097,573\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    }
   ],
   "source": [
    "tuner.get_best_models()[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in dir_2024-07-05_11-38-27\\Reviews_Classification\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 0001 summary\n",
      "Hyperparameters:\n",
      "units: 85\n",
      "num_layers: 1\n",
      "optimizer: Adam\n",
      "Score: 1.103798508644104\n",
      "\n",
      "Trial 0003 summary\n",
      "Hyperparameters:\n",
      "units: 85\n",
      "num_layers: 2\n",
      "optimizer: Adam\n",
      "Score: 1.1182894706726074\n",
      "\n",
      "Trial 0000 summary\n",
      "Hyperparameters:\n",
      "units: 85\n",
      "num_layers: 1\n",
      "optimizer: RMSprop\n",
      "Score: 1.1338261365890503\n",
      "\n",
      "Trial 0002 summary\n",
      "Hyperparameters:\n",
      "units: 85\n",
      "num_layers: 2\n",
      "optimizer: RMSprop\n",
      "Score: 1.1509822607040405\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
