{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'.\\Reviews_Dataset_Splits\\X_train.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "with open(r'.\\Reviews_Dataset_Splits\\X_val.pkl', 'rb') as f:\n",
    "    X_val = pickle.load(f)\n",
    "\n",
    "with open(r'.\\Reviews_Dataset_Splits\\X_test.pkl', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "\n",
    "with open(r'.\\Reviews_Dataset_Splits\\y_train.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "with open(r'.\\Reviews_Dataset_Splits\\y_val.pkl', 'rb') as f:\n",
    "    y_val = pickle.load(f)\n",
    "\n",
    "with open(r'.\\Reviews_Dataset_Splits\\y_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train-1, num_classes=5)\n",
    "y_val = keras.utils.to_categorical(y_val-1, num_classes=5)\n",
    "y_test = keras.utils.to_categorical(y_test-1, num_classes=5)\n",
    "\n",
    "with open(r'.\\embeddingMatrix_Reviews.pkl', 'rb') as f:\n",
    "    embedding_matrix = pickle.load(f)\n",
    "\n",
    "num_tokens = len(embedding_matrix) # total vocabulary +1 or length of embedding matrix\n",
    "embedding_dim = 300 # dimension of the vector of a single word\n",
    "MAX_REVIEW_LEN = 250 # maximum words in a review\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp, min_layers, max_layers, test_optimizers, test_initializers, test_regularizer, regularizer_choice, test_learning_rate,\n",
    "                test_activations, use_BatchNormalization, use_Dropout, units_min_value, units_max_value, units_step, same_units=False):\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        input_length=MAX_REVIEW_LEN,\n",
    "        trainable=True)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(keras.layers.GlobalAveragePooling1D())\n",
    "\n",
    "    num_layers = hp.Int('num_layers', min_layers, max_layers)\n",
    "\n",
    "    if test_activations:\n",
    "            activation = hp.Choice(f'activation', ['softplus', 'softsign', 'relu', 'tanh'])\n",
    "    else:\n",
    "        activation = 'relu'  # Default activation\n",
    "\n",
    "    if test_initializers:\n",
    "        kernel_initializer = hp.Choice(f'kernel_initializer', ['glorot_uniform', 'he_uniform', 'random_uniform'])\n",
    "    else:\n",
    "        kernel_initializer = 'he_uniform'\n",
    "\n",
    "    if test_regularizer:\n",
    "        if regularizer_choice == 'l1':\n",
    "            kernel_regularizer = keras.regularizers.L1(l1=hp.Choice('l1_factor', [1e-4, 1e-2]))\n",
    "        elif regularizer_choice == 'l2':\n",
    "            kernel_regularizer = keras.regularizers.L2(l2=hp.Choice('l2_factor', [1e-4, 1e-2]))\n",
    "        elif regularizer_choice == 'l1_l2':\n",
    "            kernel_regularizer = keras.regularizers.L1L2(l1=hp.Choice('l1_l2_l1_factor', [1e-4, 1e-2]),\n",
    "                                    l2=hp.Choice('l1_l2_l2_factor', [1e-4, 1e-2]))\n",
    "    else:\n",
    "        kernel_regularizer = None\n",
    "\n",
    "    # Hyperparameters for the number of layers\n",
    "    for i in range(num_layers):\n",
    "        if same_units:\n",
    "            current_units = units_min_value\n",
    "        else:\n",
    "            current_units = hp.Int(f'units_{i}', min_value=units_min_value, max_value=units_max_value, step=units_step)\n",
    "\n",
    "        model.add(layers.Dense(\n",
    "            units=current_units,\n",
    "            activation=activation,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            kernel_regularizer=kernel_regularizer\n",
    "        ))\n",
    "\n",
    "        if use_BatchNormalization and hp.Boolean(f'batch_norm_{i}'):\n",
    "            model.add(layers.BatchNormalization())\n",
    "\n",
    "        if use_Dropout:\n",
    "            model.add(layers.Dropout(rate=hp.Choice(f'dropout_rate_{i}', [0.0, 0.2, 0.4])))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Optimizer\n",
    "    if test_optimizers:\n",
    "        optimizer = hp.Choice('optimizer', ['SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam'])\n",
    "    else:\n",
    "        if test_learning_rate:\n",
    "            learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4, 1e-5])\n",
    "        else:\n",
    "            learning_rate = 0.001\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "directory = f'KerasTuner_Logs/FNN_V7_3Layer_Dropout{now}'\n",
    "\n",
    "# Callbacks\n",
    "tensorboard = TensorBoard(log_dir=f'TensorBoard_Logs/FNN_V7_3Layer_Dropout{now}')\n",
    "'''model_checkpoint = ModelCheckpoint(\n",
    "    filepath=f\"SavedModels/{directory}/best_model.h5\",\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")'''\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tuner = kt.GridSearch(\n",
    "    lambda hp: build_model(hp, min_layers=3, max_layers=3, test_optimizers=False, test_initializers=False, test_regularizer=False, \n",
    "                           regularizer_choice='l1_l2', test_learning_rate=False, test_activations=False, use_BatchNormalization=False, \n",
    "                           use_Dropout=True, units_min_value=96, units_max_value=96, units_step=96, same_units=True),\n",
    "    objective=kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "    max_trials=None,\n",
    "    executions_per_trial=1,\n",
    "    directory=directory,\n",
    "    project_name='Reviews_Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 27 Complete [00h 00m 26s]\n",
      "val_loss: 1.1455886363983154\n",
      "\n",
      "Best val_loss So Far: 1.0981272459030151\n",
      "Total elapsed time: 00h 15m 15s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x=X_train,\n",
    "             y=y_train,\n",
    "             verbose=1,\n",
    "             epochs=NUM_EPOCHS,\n",
    "             batch_size=BATCH_SIZE,\n",
    "             callbacks=[tensorboard, early_stopping],\n",
    "             validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_layers': 3,\n",
       " 'dropout_rate_0': 0.4,\n",
       " 'dropout_rate_1': 0.2,\n",
       " 'dropout_rate_2': 0.4}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 250, 300)          16058400  \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 300)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 96)                28896     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 96)                9312      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 96)                9312      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 485       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,106,405\n",
      "Trainable params: 16,106,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    }
   ],
   "source": [
    "tuner.get_best_models()[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in KerasTuner_Logs/FNN_V4_3Layer_Regularizers_2024-07-09_12-41-45\\Reviews_Classification\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 0001 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "kernel_regularizer: l2\n",
      "Score: 1.609480857849121\n",
      "\n",
      "Trial 0002 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "kernel_regularizer: l1_l2\n",
      "Score: 1.6679480075836182\n",
      "\n",
      "Trial 0000 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "kernel_regularizer: l1\n",
      "Score: 1.6689815521240234\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from keras.models import load_model\n",
    "\n",
    "# Specify the path to your .h5 model file\n",
    "model_path = 'SavedModels/dir_2024-07-04_16-19-08/best_model.h5'\n",
    "\n",
    "# Load the model\n",
    "s_model = load_model(model_path)\n",
    "s_model.optimizer.get_config()'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
