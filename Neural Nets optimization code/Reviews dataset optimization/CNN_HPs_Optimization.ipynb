{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'.\\Reviews_Dataset_Splits\\X_train.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "with open(r'.\\Reviews_Dataset_Splits\\X_val.pkl', 'rb') as f:\n",
    "    X_val = pickle.load(f)\n",
    "\n",
    "with open(r'.\\Reviews_Dataset_Splits\\X_test.pkl', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "\n",
    "with open(r'.\\Reviews_Dataset_Splits\\y_train.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "with open(r'.\\Reviews_Dataset_Splits\\y_val.pkl', 'rb') as f:\n",
    "    y_val = pickle.load(f)\n",
    "\n",
    "with open(r'.\\Reviews_Dataset_Splits\\y_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train-1, num_classes=5)\n",
    "y_val = keras.utils.to_categorical(y_val-1, num_classes=5)\n",
    "y_test = keras.utils.to_categorical(y_test-1, num_classes=5)\n",
    "\n",
    "with open(r'.\\embeddingMatrix_Reviews.pkl', 'rb') as f:\n",
    "    embedding_matrix = pickle.load(f)\n",
    "\n",
    "num_tokens = len(embedding_matrix) # total vocabulary +1 or length of embedding matrix\n",
    "embedding_dim = 300 # dimension of the vector of a single word\n",
    "MAX_REVIEW_LEN = 250 # maximum words in a review\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp, min_layers, max_layers, test_optimizers, test_initializers, test_regularizer, regularizer_choice, test_learning_rate,\n",
    "                test_activations, use_BatchNormalization, use_Dropout, filters_min_value, filters_max_value, filters_step, same_filters):\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        input_length=MAX_REVIEW_LEN,\n",
    "        trainable=True)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(embedding_layer)\n",
    "\n",
    "    num_layers = hp.Int('num_layers', min_layers, max_layers)\n",
    "\n",
    "    if test_activations:\n",
    "            activation = hp.Choice(f'activation', ['softplus', 'softsign', 'relu', 'tanh'])\n",
    "    else:\n",
    "        activation = 'relu'  # Default activation\n",
    "\n",
    "    if test_initializers:\n",
    "        kernel_initializer = hp.Choice(f'kernel_initializer', ['glorot_uniform', 'he_uniform', 'random_uniform'])\n",
    "    else:\n",
    "        kernel_initializer = 'he_uniform'\n",
    "\n",
    "    if test_regularizer:\n",
    "        if regularizer_choice == 'l1':\n",
    "            kernel_regularizer = keras.regularizers.L1(l1=hp.Choice('l1_factor', [1e-4, 1e-2]))\n",
    "        elif regularizer_choice == 'l2':\n",
    "            kernel_regularizer = keras.regularizers.L2(l2=hp.Choice('l2_factor', [1e-4, 1e-2]))\n",
    "        elif regularizer_choice == 'l1_l2':\n",
    "            kernel_regularizer = keras.regularizers.L1L2(l1=hp.Choice('l1_l2_l1_factor', [1e-4, 1e-2]),\n",
    "                                    l2=hp.Choice('l1_l2_l2_factor', [1e-4, 1e-2]))\n",
    "    else:\n",
    "        kernel_regularizer = None\n",
    "\n",
    "    # Hyperparameters for the number of layers\n",
    "    for i in range(num_layers):\n",
    "        if same_filters:\n",
    "            filters = filters_min_value\n",
    "        else:\n",
    "            filters = hp.Int(f'filters_{i}', min_value=filters_min_value, max_value=filters_max_value, step=filters_step)\n",
    "\n",
    "        model.add(layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=5,\n",
    "            activation=activation,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            padding='same'\n",
    "        ))\n",
    "\n",
    "        if use_BatchNormalization and hp.Boolean(f'batch_norm_{i}'):\n",
    "            model.add(layers.BatchNormalization())\n",
    "\n",
    "        model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "        if use_Dropout:\n",
    "            model.add(layers.Dropout(rate=hp.Choice(f'dropout_rate_{i}', [0.0, 0.2, 0.4])))\n",
    "\n",
    "        model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Optimizer\n",
    "    if test_optimizers:\n",
    "        optimizer = hp.Choice('optimizer', ['SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam'])\n",
    "    else:\n",
    "        if test_learning_rate:\n",
    "            learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4, 1e-5])\n",
    "        else:\n",
    "            learning_rate = 0.001\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 12\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "directory = f'KerasTuner_Logs/CNN/CNN_V2_3Layer_Optimizer_{now}'\n",
    "\n",
    "# Callbacks\n",
    "tensorboard = TensorBoard(log_dir=f'TensorBoard_Logs/CNN/CNN_V2_3Layer_Optimizer_{now}')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "\n",
    "tuner = kt.GridSearch(\n",
    "    lambda hp: build_model(hp, min_layers=3, max_layers=3, test_optimizers=True, test_initializers=False, test_regularizer=False, \n",
    "                           regularizer_choice='l1', test_learning_rate=False, test_activations=False, use_BatchNormalization=False, \n",
    "                           use_Dropout=False, filters_min_value=128, filters_max_value=128, filters_step=128, same_filters=True),\n",
    "    objective=kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "    max_trials=None,\n",
    "    executions_per_trial=1,\n",
    "    directory=directory,\n",
    "    project_name='Reviews_Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pd(hp, test_optimizers, test_initializers, test_regularizer, regularizer_choice, test_learning_rate,\n",
    "                test_activations):\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        input_length=MAX_REVIEW_LEN,\n",
    "        trainable=True)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(embedding_layer)\n",
    "\n",
    "    if test_activations:\n",
    "        activation_choice = hp.Choice('activation', ['softplus', 'softsign', 'relu', 'tanh'])\n",
    "        activation = tf.keras.activations.get(activation_choice)\n",
    "    else:\n",
    "        activation = 'relu'  # Default activation\n",
    "\n",
    "    if test_initializers:\n",
    "        initializer_choice = hp.Choice('kernel_initializer', ['glorot_uniform', 'he_uniform', 'random_uniform'])\n",
    "        kernel_initializer = tf.keras.initializers.get(initializer_choice)\n",
    "    else:\n",
    "        kernel_initializer = 'he_uniform'\n",
    "\n",
    "    if test_regularizer:\n",
    "        if regularizer_choice == 'l1':\n",
    "            kernel_regularizer = keras.regularizers.L1(l1=hp.Choice('l1_factor', [1e-4, 1e-2]))\n",
    "        elif regularizer_choice == 'l2':\n",
    "            kernel_regularizer = keras.regularizers.L2(l2=hp.Choice('l2_factor', [1e-4, 1e-2]))\n",
    "        elif regularizer_choice == 'l1_l2':\n",
    "            kernel_regularizer = keras.regularizers.L1L2(l1=hp.Choice('l1_l2_l1_factor', [1e-4, 1e-2]),\n",
    "                                    l2=hp.Choice('l1_l2_l2_factor', [1e-4, 1e-2]))\n",
    "    else:\n",
    "        kernel_regularizer = None\n",
    "\n",
    "    model.add(layers.Conv1D(filters=96, kernel_size=5, activation=activation,\n",
    "            kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer, padding='same'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(layers.Conv1D(filters=96, kernel_size=5, activation=activation,\n",
    "            kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer, padding='same'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    \n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=5, activation=activation,\n",
    "            kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Optimizer\n",
    "    if test_optimizers:\n",
    "        optimizer = hp.Choice('optimizer', ['SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam'])\n",
    "    else:\n",
    "        if test_learning_rate:\n",
    "            learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4, 1e-5])\n",
    "        else:\n",
    "            learning_rate = 0.001\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 25\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "directory = f'KerasTuner_Logs/CNN/CNN_V6_3Layer_LearningRate_{now}'\n",
    "\n",
    "# Callbacks\n",
    "tensorboard = TensorBoard(log_dir=f'TensorBoard_Logs/CNN/CNN_V6_3Layer_LearningRate_{now}')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "\n",
    "tuner = kt.GridSearch(\n",
    "    lambda hp: build_model_pd(hp, test_optimizers=False, test_initializers=False, test_regularizer=False, \n",
    "                           regularizer_choice='l2', test_learning_rate=True, test_activations=False),\n",
    "    objective=kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "    max_trials=None,\n",
    "    executions_per_trial=1,\n",
    "    directory=directory,\n",
    "    project_name='Reviews_Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pd2(hp, test_optimizers, test_initializers, test_regularizer, regularizer_choice, test_learning_rate,\n",
    "                   test_activations, test_batch_norm):\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        input_length=MAX_REVIEW_LEN,\n",
    "        trainable=True)\n",
    "   \n",
    "    model = keras.Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    \n",
    "    if test_activations:\n",
    "        activation_choice = hp.Choice('activation', ['softplus', 'softsign', 'relu', 'tanh'])\n",
    "        activation = tf.keras.activations.get(activation_choice)\n",
    "    else:\n",
    "        activation = 'relu'\n",
    "    \n",
    "    if test_initializers:\n",
    "        initializer_choice = hp.Choice('kernel_initializer', ['glorot_uniform', 'he_uniform', 'random_uniform'])\n",
    "        kernel_initializer = tf.keras.initializers.get(initializer_choice)\n",
    "    else:\n",
    "        kernel_initializer = 'he_uniform'\n",
    "    \n",
    "    if test_regularizer:\n",
    "        if regularizer_choice == 'l1':\n",
    "            kernel_regularizer = keras.regularizers.L1(l1=hp.Choice('l1_factor', [1e-4, 1e-2]))\n",
    "        elif regularizer_choice == 'l2':\n",
    "            kernel_regularizer = keras.regularizers.L2(l2=hp.Choice('l2_factor', [1e-4, 1e-2]))\n",
    "        elif regularizer_choice == 'l1_l2':\n",
    "            kernel_regularizer = keras.regularizers.L1L2(l1=hp.Choice('l1_l2_l1_factor', [1e-4, 1e-2]),\n",
    "                                    l2=hp.Choice('l1_l2_l2_factor', [1e-4, 1e-2]))\n",
    "    else:\n",
    "        kernel_regularizer = None\n",
    "\n",
    "    # Function to add a convolutional block\n",
    "    def add_conv_block(filters, dropout_rate=0.2):\n",
    "        model.add(layers.Conv1D(filters=filters, kernel_size=5, activation=activation,\n",
    "                kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer, padding='same'))\n",
    "        if test_batch_norm:\n",
    "            use_bn = hp.Boolean(f'batch_norm_after_conv_{filters}')\n",
    "            if use_bn:\n",
    "                model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling1D(pool_size=2))\n",
    "        if dropout_rate > 0:\n",
    "            model.add(layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    # Add convolutional blocks\n",
    "    add_conv_block(96)\n",
    "    add_conv_block(96)\n",
    "    add_conv_block(64, dropout_rate=0)\n",
    "\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Optimizer\n",
    "    if test_optimizers:\n",
    "        optimizer = hp.Choice('optimizer', ['SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam'])\n",
    "    else:\n",
    "        if test_learning_rate:\n",
    "            learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4, 1e-5])\n",
    "        else:\n",
    "            learning_rate = 0.001\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 25\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "directory = f'KerasTuner_Logs/CNN/CNN_V7_3Layer_BatchNormalization_{now}'\n",
    "\n",
    "# Callbacks\n",
    "tensorboard = TensorBoard(log_dir=f'TensorBoard_Logs/CNN/CNN_V7_3Layer_BatchNormalization_{now}')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "\n",
    "tuner = kt.GridSearch(\n",
    "    lambda hp: build_model_pd2(hp, test_optimizers=False, test_initializers=False, test_regularizer=False, \n",
    "                           regularizer_choice='l2', test_learning_rate=False, test_activations=False, test_batch_norm=True),\n",
    "    objective=kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "    max_trials=None,\n",
    "    executions_per_trial=1,\n",
    "    directory=directory,\n",
    "    project_name='Reviews_Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_do(hp, use_dropout):\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        input_length=MAX_REVIEW_LEN,\n",
    "        trainable=True)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(embedding_layer)\n",
    "\n",
    "    activation = 'relu'  # Default activation\n",
    "    kernel_initializer = 'he_uniform'\n",
    "    kernel_regularizer = None\n",
    "\n",
    "    model.add(layers.Conv1D(filters=96, kernel_size=5, activation=activation,\n",
    "            kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer, padding='same'))\n",
    "    if use_dropout:\n",
    "            model.add(layers.Dropout(rate=hp.Choice(f'dropout_rate_1', [0.0, 0.2, 0.4])))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(layers.Conv1D(filters=96, kernel_size=5, activation=activation,\n",
    "            kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer, padding='same'))\n",
    "    if use_dropout:\n",
    "            model.add(layers.Dropout(rate=hp.Choice(f'dropout_rate_2', [0.0, 0.2, 0.4])))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=5, activation=activation,\n",
    "            kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    if use_dropout:\n",
    "            model.add(layers.Dropout(rate=hp.Choice(f'dropout_rate_3', [0.0, 0.2, 0.4])))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 15\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "directory = f'KerasTuner_Logs/CNN/CNN_V8_3Layer_DropOut_{now}'\n",
    "\n",
    "# Callbacks\n",
    "tensorboard = TensorBoard(log_dir=f'TensorBoard_Logs/CNN/CNN_V8_3Layer_DropOut_{now}')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "\n",
    "tuner = kt.GridSearch(\n",
    "    lambda hp: build_model_do(hp, use_dropout=True),\n",
    "    objective=kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "    max_trials=None,\n",
    "    executions_per_trial=1,\n",
    "    directory=directory,\n",
    "    project_name='Reviews_Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 27 Complete [00h 01m 17s]\n",
      "val_loss: 1.0673409700393677\n",
      "\n",
      "Best val_loss So Far: 1.0247337818145752\n",
      "Total elapsed time: 00h 30m 39s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x=X_train,\n",
    "             y=y_train,\n",
    "             verbose=1,\n",
    "             epochs=NUM_EPOCHS,\n",
    "             batch_size=BATCH_SIZE,\n",
    "             callbacks=[tensorboard, early_stopping],\n",
    "             validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout_rate_1': 0.0, 'dropout_rate_2': 0.4, 'dropout_rate_3': 0.2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 250, 300)          16058400  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 250, 96)           144096    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 250, 96)           0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 125, 96)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 125, 96)           46176     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 125, 96)           0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 62, 96)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 62, 64)            30784     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 62, 64)           256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 62, 64)            0         \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 31, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 64)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,280,037\n",
      "Trainable params: 16,279,909\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    }
   ],
   "source": [
    "tuner.get_best_models()[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in KerasTuner_Logs/CNN/CNN_V5_3Layer_RegularizerL2_2024-07-13_13-28-40\\Reviews_Classification\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 0000 summary\n",
      "Hyperparameters:\n",
      "l2_factor: 0.0001\n",
      "Score: 1.1286063194274902\n",
      "\n",
      "Trial 0001 summary\n",
      "Hyperparameters:\n",
      "l2_factor: 0.01\n",
      "Score: 1.6093945503234863\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
