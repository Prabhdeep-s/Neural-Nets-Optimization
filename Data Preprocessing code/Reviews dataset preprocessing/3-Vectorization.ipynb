{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "from fasttext import load_model\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"..\\Dataset1\\AFF_Reviews_tokenized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145150"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>long time fan original chip ahoy cooky like si...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>really tasty chip lifetime say delectable one ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vendor clearly attempt take advantage consumer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>order original beef jerky ounce bags pack get ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>try find drink replace soda drink daily like t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>best cooky ever eat would never know sugar fre...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>far favorite brand soynut butter taste yummy i...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>little disappointed particular offering usuall...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>really much confidence commercial pet food ing...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>saw available vine program compel order simply...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>order birthday get birthday money family order...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>small sized bottle perfect someone would like ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>favorite smoked salmon amazon overly fishy tas...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>delicious yes confuse relatively small bar pac...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pleasantly surprise fresh taste orange bar rea...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>enjoy pasta health benefit family also like br...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>disclaimer brew coffee maker use pod term tast...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gluten free year first try product usually mak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mind price set contain everything need get sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>huge coffee drinker husband like strong bold c...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  Score\n",
       "0   long time fan original chip ahoy cooky like si...      3\n",
       "1   really tasty chip lifetime say delectable one ...      5\n",
       "2   vendor clearly attempt take advantage consumer...      1\n",
       "3   order original beef jerky ounce bags pack get ...      1\n",
       "4   try find drink replace soda drink daily like t...      3\n",
       "5   best cooky ever eat would never know sugar fre...      4\n",
       "6   far favorite brand soynut butter taste yummy i...      5\n",
       "7   little disappointed particular offering usuall...      3\n",
       "8   really much confidence commercial pet food ing...      3\n",
       "9   saw available vine program compel order simply...      1\n",
       "10  order birthday get birthday money family order...      2\n",
       "11  small sized bottle perfect someone would like ...      4\n",
       "12  favorite smoked salmon amazon overly fishy tas...      2\n",
       "13  delicious yes confuse relatively small bar pac...      3\n",
       "14  pleasantly surprise fresh taste orange bar rea...      4\n",
       "15  enjoy pasta health benefit family also like br...      5\n",
       "16  disclaimer brew coffee maker use pod term tast...      4\n",
       "17  gluten free year first try product usually mak...      1\n",
       "18  mind price set contain everything need get sta...      4\n",
       "19  huge coffee drinker husband like strong bold c...      3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in 'Text' column: 1\n"
     ]
    }
   ],
   "source": [
    "nan_count = data['Text'].isna().sum()\n",
    "print(f\"Number of NaN values in 'Text' column: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = data.dropna(subset=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in 'Text' column: 0\n"
     ]
    }
   ],
   "source": [
    "nan_count = df_cleaned['Text'].isna().sum()\n",
    "print(f\"Number of NaN values in 'Text' column: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29030\n",
      "29030\n",
      "29030\n",
      "29030\n",
      "29029\n",
      "145149\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    print((df_cleaned['Score'] == i).sum())\n",
    "\n",
    "print(len(df_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxRowsToKeep = 29000\n",
    "for i in range(1,6):\n",
    "    rows_with_value = df_cleaned[df_cleaned['Score'] == i]\n",
    "    rows_to_remove = rows_with_value.sample(n=(len(rows_with_value)-maxRowsToKeep))\n",
    "    df_cleaned = df_cleaned.drop(rows_to_remove.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29000\n",
      "29000\n",
      "29000\n",
      "29000\n",
      "29000\n",
      "145000\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    print((df_cleaned['Score'] == i).sum())\n",
    "\n",
    "print(len(df_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53527\n"
     ]
    }
   ],
   "source": [
    "def find_unique_words(strings):\n",
    "        combined_string = ' '.join(strings)\n",
    "        words = combined_string.split()\n",
    "        unique_words = set(words)\n",
    "        return unique_words\n",
    "\n",
    "unique_words = find_unique_words(df_cleaned[\"Text\"])\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vectorizer model\n",
    "vectorizer = gensim.models.KeyedVectors.load_word2vec_format(r'..\\Vectorizer\\FastText-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the keras tokenizer on the entire dataset\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df_cleaned['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53527"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# + 1 to account for padding token.\n",
    "num_tokens = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Initialize a matrix of zeroes of size: vocabulary x embedding dimension.\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "  if vectorizer.has_index_for(word):\n",
    "    embedding_matrix[i] = vectorizer[word].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0129 -0.0311  0.0133  0.0051 -0.0395 -0.0044 -0.0218 -0.0483  0.021\n",
      "  0.0186 -0.0313  0.0012  0.0194 -0.0124  0.0116 -0.0149  0.0489  0.0029\n",
      "  0.0437 -0.0069 -0.0129  0.0165 -0.0162  0.0322  0.0181 -0.01    0.0173\n",
      " -0.0312  0.0552 -0.0006 -0.0004 -0.0177  0.0048 -0.0616  0.0065 -0.0015\n",
      "  0.0203 -0.0142 -0.0047  0.0054  0.0096  0.0071 -0.0081 -0.0085 -0.0088\n",
      "  0.0129  0.0017 -0.0259  0.0174  0.0354]\n"
     ]
    }
   ],
   "source": [
    "# Quick check\n",
    "print(embedding_matrix[tokenizer.word_index['great']][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'..\\Dataset1\\embeddingMatrixDS1.pkl', 'wb') as f:\n",
    "    pickle.dump(embedding_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_cleaned['Text'], np.array(df_cleaned['Score']), test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 19811, 3: 19773, 2: 19729, 4: 19667, 5: 19620})\n",
      "Counter({4: 4996, 5: 4962, 2: 4929, 1: 4909, 3: 4854})\n",
      "Counter({5: 4418, 3: 4373, 2: 4342, 4: 4337, 1: 4280})\n"
     ]
    }
   ],
   "source": [
    "print(collections.Counter(y_train))\n",
    "print(collections.Counter(y_val))\n",
    "print(collections.Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115, 28, 169, 2, 3, 219, 1082, 148, 314, 5, 314, 119, 8, 662, 9, 589]\n"
     ]
    }
   ],
   "source": [
    "print(X_val_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['k', 'cup', 'delicious', 'taste', 'good']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.index_word[x] for x in X_val_seq[0][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k cup delicious taste good save alot money cream product cream right coffee plan buy soon'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([X_val_seq[0]])[0][:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row with the highest word count:\n",
      "Score: 2\n",
      "Word Count: 249\n",
      "Text (first 100 characters): leave general mill undo nature make decision ingredient iexcl ingredient vanilla ingredient roll oat...\n"
     ]
    }
   ],
   "source": [
    "def count_words(text):\n",
    "    return len(str(text).split())\n",
    "\n",
    "# Apply the function to the 'Text' column\n",
    "df_cleaned['Word_Count'] = df_cleaned['Text'].apply(count_words)\n",
    "\n",
    "# Find the row with the maximum word count\n",
    "max_word_count_row = df_cleaned.loc[df_cleaned['Word_Count'].idxmax()]\n",
    "\n",
    "print(\"Row with the highest word count:\")\n",
    "print(f\"Score: {max_word_count_row['Score']}\")\n",
    "print(f\"Word Count: {max_word_count_row['Word_Count']}\")\n",
    "print(f\"Text (first 100 characters): {max_word_count_row['Text'][:100]}...\")\n",
    "\n",
    "# If you want to see all rows with the maximum word count (in case of ties):\n",
    "max_word_count = df_cleaned['Word_Count'].max()\n",
    "max_word_count_rows = df_cleaned[df_cleaned['Word_Count'] == max_word_count]\n",
    "\n",
    "if len(max_word_count_rows) > 1:\n",
    "    print(f\"\\nThere are {len(max_word_count_rows)} rows with the maximum word count of {max_word_count}.\")\n",
    "    for index, row in max_word_count_rows.iterrows():\n",
    "        print(f\"\\nScore: {row['Score']}\")\n",
    "        print(f\"Word Count: {row['Word_Count']}\")\n",
    "        print(f\"Text (first 100 characters): {row['Text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS_LEN = 250\n",
    "X_train_padded = keras.preprocessing.sequence.pad_sequences(X_train_seq, maxlen=MAX_TOKENS_LEN, padding='post')\n",
    "X_val_padded = keras.preprocessing.sequence.pad_sequences(X_val_seq, maxlen=MAX_TOKENS_LEN, padding='post')\n",
    "X_test_padded = keras.preprocessing.sequence.pad_sequences(X_test_seq, maxlen=MAX_TOKENS_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'..\\Dataset1\\Dataset1Splits\\X_train.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train_padded, f)\n",
    "\n",
    "with open(r'..\\Dataset1\\Dataset1Splits\\X_val.pkl', 'wb') as f:\n",
    "    pickle.dump(X_val_padded, f)\n",
    "\n",
    "with open(r'..\\Dataset1\\Dataset1Splits\\X_test.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test_padded, f)\n",
    "\n",
    "with open(r'..\\Dataset1\\Dataset1Splits\\y_train.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "\n",
    "with open(r'..\\Dataset1\\Dataset1Splits\\y_val.pkl', 'wb') as f:\n",
    "    pickle.dump(y_val, f)\n",
    "\n",
    "with open(r'..\\Dataset1\\Dataset1Splits\\y_test.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
